/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package fi.aalto.hacid;

import java.io.IOException;
import java.util.Collection;
import java.util.LinkedList;
import java.util.List;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HColumnDescriptor;
import org.apache.hadoop.hbase.HConstants;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.KeyValue;
import org.apache.hadoop.hbase.client.Delete;
import org.apache.hadoop.hbase.client.Get;
import org.apache.hadoop.hbase.client.HBaseAdmin;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.ResultScanner;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.filter.CompareFilter;
import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.log4j.Logger;

/**
 * Provides interaction with a HAcid system in a HBase site, offering
 * transaction processing.
 * 
 * <p>
 * Example:
 * 
 * <p>
 * <code>
 * Configuration conf = HBaseConfiguration.create();<br />
 * HAcidClient client = new HAcidClient(conf); <br />
 * HAcidTable mytable = new HAcidTable(conf, "mytable"); <br />
 * <br />
 * HAcidTxn txn = new HAcidTxn(client); <br />
 * HAcidPut p1 = new HAcidPut(mytable, Bytes.toBytes("row1")); <br />
 * p1.add(Bytes.toBytes("fam"),Bytes.toBytes("col1"),Bytes.toBytes("some_value")); <br />
 * HAcidPut p2 = new HAcidPut(mytable, Bytes.toBytes("row2")); <br />
 * p2.add(Bytes.toBytes("fam"),Bytes.toBytes("col1"),Bytes.toBytes("some_other_value")); <br />
 * txn.put(p1); <br />
 * txn.put(p2); <br />
 * txn.commit(); <br />
 * <br />
 * // ...<br />
 * <br />
 * mytable.close(); <br />
 * client.close();<br />
 * </code>
 * 
 * @author Andre Medeiros <andre.medeiros@aalto.fi>
 */
public class HAcidClient {

  static Logger LOG = Logger.getLogger(HAcidClient.class.getName());

  private HTable timestamp_log_table;
  Configuration configuration_HBase;
  private static int SCAN_TIMESTAMP_MAX_CACHE = 1024; // max num. rows in scan

  public enum IsolationLevel {
    /**
     * Snapshot Isolation is an Isolation Level where reads in transactions
     * operate on a "snapshot" of all data at the beginning of the transaction.
     * This Isolation Level does not necessarily produce serializable histories. <br />
     * <br />
     * This is the default Isolation Level in HAcid.
     */
    SNAPSHOT_ISOLATION,
    /**
     * Write-Snapshot Isolation ("A Critique of Snapshot Isolation", Ferro &
     * Yabandeh) is an Isolation Level that provides Serializability. Operates
     * in a similar way as Snapshot Isolation, but detects read-write conflicts
     * instead of write-write conflicts. Gives Serializability at the cost of
     * minor performance drawbacks.
     */
    WRITE_SNAPSHOT_ISOLATION;
  }

  private IsolationLevel isolation = IsolationLevel.SNAPSHOT_ISOLATION;

  /**
   * @return the Isolation Level to deal with transactions.
   */
  public IsolationLevel getIsolation() {
    return isolation;
  }

  /**
   * Sets the isolation level for transactions in HAcid. <br />
   * <br />
   * WARNING: it is the programmer's responsibility to ensure that all
   * concurrent HAcid clients are using the same isolation level. Moreover, The
   * programmer is also responsible for ensuring that no HAcid clients are
   * actively managing transactions when the isolation level is modified.
   * Otherwise, some transactions may lose their consistency and isolation
   * properties. HAcid does not perform distributed consensus of the new
   * isolation level. <br />
   * <br />
   * Therefore, it is recommended to set the isolation level only at the
   * beginning of HAcid's use.
   * 
   * @param isolation
   *          the Isolation Level to deal with transactions.
   */
  public void setIsolation(IsolationLevel isolation) {
    this.isolation = isolation;
  }

  /**
   * Initializes a HAcid client instance. If necessary, installs HAcid in the
   * HBase site, creating the HAcid metadata tables.
   * 
   * @param config
   *          The configuration data of the HBase site where HAcid metadata
   *          tables will stay.
   * @throws Exception
   */
  public HAcidClient(Configuration config) throws Exception {
    if (!isInstalled(config)) {
      install(config, null);
    }
    initialize(config);
  }

  /**
   * Initializes a HAcid client instance. If necessary, installs HAcid in the
   * HBase site, creating the HAcid metadata tables, and prepares each user
   * table given by <code>allUserTables</code> to be used by HAcid.
   * 
   * @param config
   *          The configuration data of the HBase site where HAcid metadata
   *          tables will stay.
   * @param allUserTables
   *          A Collection of user tables to be prepared for use in HAcid.
   * @throws Exception
   */
  public HAcidClient(Configuration config, Collection<HTable> allUserTables)
      throws Exception {
    if (!isInstalled(config)) {
      install(config, allUserTables);
    }
    initialize(config);
  }

  /**
   * Prepare this client for using HAcid metadata tables at HBase site given by
   * config.
   * 
   * @throws Exception
   */
  private void initialize(Configuration config) throws Exception {
    LOG.debug("Initializing HAcid client ...");

    if (config == null) {
      LOG.error("No connection to HBase. Configuration is null");
    }
    configuration_HBase = config;

    if (timestamp_log_table != null) {
      timestamp_log_table.close();
      timestamp_log_table = null;
    }
    timestamp_log_table = new HTable(config, Schema.TABLE_TIMESTAMP_LOG);
  }

  /**
   * At the end of this HAcid client session, call this to properly end the
   * client interaction.
   * 
   * @throws IOException
   */
  public void close() throws IOException {
    LOG.debug("Closing HAcid client ...");

    timestamp_log_table.close();
  }

  static byte[] timestampToKey(byte[] timestampBytes) {
    return timestampToKey(Bytes.toLong(timestampBytes));
  }

  static byte[] timestampToKey(long timestamp) {
    return Bytes.toBytes((Long.MAX_VALUE - timestamp));
  }

  static long keyToTimestamp(byte[] timestamp_log_row_key) {
    return keyToTimestamp(Bytes.toLong(timestamp_log_row_key));
  }

  static long keyToTimestamp(long timestamp_log_row_key) {
    return (Long.MAX_VALUE - timestamp_log_row_key);
  }

  /**
   * This describes a Put to be performed atomically at the same time HAcid
   * retrieves a timestamp.
   */
  private interface ActionOnGrab {
    Put makePut(byte[] rowkey);
  }

  /**
   * Describes what kind of Put should be done on the row that gives a new start
   * timestamp.
   */
  private class StartTimestampActionOnGrab implements ActionOnGrab {
    @Override
    public Put makePut(byte[] rowkey) {
      Put p = new Put(rowkey);
      p.add(Schema.FAMILY_HACID, Schema.QUALIFIER_TS_TYPE, Schema.TYPE_START);
      p.add(Schema.FAMILY_HACID, Schema.QUALIFIER_TXN_STATE,
          Schema.STATE_ACTIVE);
      return p;
    }
  }

  /**
   * Describes what kind of Put should be done on the Timestamp-Log row that
   * gives a new end timestamp.
   */
  private class EndTimestampActionOnGrab implements ActionOnGrab {
    private HAcidTxn _txn;

    public EndTimestampActionOnGrab(HAcidTxn txn) {
      _txn = txn;
    }

    @Override
    public Put makePut(byte[] rowkey) {
      Put p = new Put(rowkey);

      p.add(Schema.FAMILY_HACID, Schema.QUALIFIER_TS_TYPE, Schema.TYPE_END);

      p.add(Schema.FAMILY_HACID, Schema.QUALIFIER_START_TIMESTAMP,
          Bytes.toBytes(_txn.getStartTimestamp()));

      Collection<byte[]> writesetRows = _txn.getWriteset();
      for (byte[] rowId : writesetRows) {
        LOG.trace(_txn.toStringHash() + " writes cell " + Bytes.toString(rowId));
        p.add(Schema.FAMILY_WRITESET, rowId, Schema.MARKER_TRUE);
      }

      Collection<byte[]> readsetRows = _txn.getReadset();
      for (byte[] rowId : readsetRows) {
        LOG.trace(_txn.toStringHash() + " reads cell " + Bytes.toString(rowId));
        p.add(Schema.FAMILY_READSET, rowId, Schema.MARKER_TRUE);
      }

      return p;
    }
  }

  /**
   * Retrieves a new start timestamp from HAcid for some transaction, and
   * (atomically) at the same time registers the 'active' state for that txn.
   * 
   * @return
   * @throws IOException
   */
  long requestStartTimestamp() throws IOException {
    return grabTimestamp(new StartTimestampActionOnGrab());
  }

  /**
   * Retrieves an end timestamp from HAcid for the transaction <code>txn</code>,
   * and (atomically) at the same time registers the write set of
   * <code>txn</code>.
   * 
   * @param txn
   * @return
   * @throws IOException
   */
  long requestEndTimestamp(HAcidTxn txn) throws IOException {
    return grabTimestamp(new EndTimestampActionOnGrab(txn));
  }

  /**
   * How many steps is the grabTimestamp method allowed to take before skipping
   * to the beginning of the Timestamp Log. A 'step' is a failed checkAndPut for
   * a timestamp one row above the previous attempt.
   */
  public static int GRAB_TIMESTAMP_MAX_STEPS_BEFORE_SKIPPING_TO_BEGINNING = 26;

  // 26 was determined by experimenting values, and it is a local minimum
  // i.e., 26 renders a local minimum of number of invocations to checkAndPut,
  // and also average time spent in grabTimestamp.

  /**
   * Atomically creates a new timestamp in HAcid and applies
   * <code>postGrabPut</code> on that timestamp.
   * 
   * @param postGrabPut
   * @return
   * @throws IOException
   */
  private long grabTimestamp(ActionOnGrab action) throws IOException {
    if (timestamp_log_table == null) {
      LOG.error("HAcid Timestamp-Log table is not initialized");
    }
    if (action == null) {
      LOG.error("Null ActionOnGrab given to grabTimestamp()");
    }

    long firstRowKey = Bytes
        .toLong(timestampToKey(Schema.TIMESTAMP_INITIAL_LONG));
    int steps = GRAB_TIMESTAMP_MAX_STEPS_BEFORE_SKIPPING_TO_BEGINNING;

    boolean firstIsAvailable; // "available" means "inexistent row"
    do {
      if (steps >= GRAB_TIMESTAMP_MAX_STEPS_BEFORE_SKIPPING_TO_BEGINNING) {
        steps = 0;
        // Get the first row
        Scan scan = new Scan();
        ResultScanner scanner = timestamp_log_table.getScanner(scan);
        Result firstRowRes = scanner.next();
        scanner.close();
        scanner = null;
        scan = null;
        if (firstRowRes == null) {
          LOG.fatal("HAcid Timestamp Log is empty.");
        }
        firstRowKey = Bytes.toLong(firstRowRes.getRow()) - 1L;
      }

      // Checks if the first row is available, and if true does the Put
      firstIsAvailable = timestamp_log_table.checkAndPut(
          Bytes.toBytes(firstRowKey), Schema.FAMILY_HACID,
          Schema.QUALIFIER_TS_TYPE, null,
          action.makePut(Bytes.toBytes(firstRowKey)));

      // Prepares to retry to grab the timestamp from a newly created row
      if (!firstIsAvailable) {
        steps++;
        firstRowKey--;
        if (firstRowKey <= 0L) {
          LOG.fatal("Ran out of timestamps.");
        }
      }
    }
    while (!firstIsAvailable);

    return keyToTimestamp(firstRowKey);
  }

  /**
   * Returns row data at the proper timestamp with regard to the reference
   * timestamp given.
   * 
   * If the Isolation Level is Snapshot Isolation, returns row data at the S.I.
   * Read Timestamp of the given <code>row</code> respecting the reference
   * timestamp <code>ref_timestamp</code>.
   * 
   * <p>
   * Given the reference timestamp <code>ref_timestamp</code> (usually the
   * start-timestamp of the txn R that is performing the read), and a user data
   * row <code>row</code>, the Snapshot Isolated Read TimestampÂ is defined as
   * T.start-ts of the transaction T that wrote to <code>row</code>, with
   * T.end-ts as close as possible to <code>ref_timestamp</code>, but still
   * T.end-ts < <code>ref_timestamp</code>. If R wrote to <code>row</code>
   * before doing this search, then Snapshot Isolated Read Timestamp is defined
   * as R.start-ts.
   * 
   * @param usertable
   * @param row
   * @param timestamp
   * @return
   * @throws IOException
   */
  Result getFromReadTimestamp(HAcidGet hacidget, long ref_timestamp)
      throws IOException {
    LOG.debug("Getting Snapshot Isolated Read Timestamp of "
        + Bytes.toString(hacidget.hacidtable.getTableName()) + ":"
        + Bytes.toString(hacidget.getRow()) + " until timestamp "
        + ref_timestamp);

    // Get the specified user row, all timestamps
    Get get = new Get(hacidget.getRow());
    get.setMaxVersions(); // get all versions
    hacidget.sendToGet(get); // add user columns
    get.addColumn(Schema.FAMILY_HACID, Schema.QUALIFIER_USERTABLE_COMMITTED_AT); // add
                                                                                 // txn
                                                                                 // metadata
                                                                                 // column
    Result res = hacidget.hacidtable.get(get);
    List<KeyValue> list_committed_at = res.getColumn(Schema.FAMILY_HACID,
        Schema.QUALIFIER_USERTABLE_COMMITTED_AT);

    LOG.trace("Timestamp given: " + ref_timestamp);

    long best_timestamp = 0L; // for finding write snapshot isolated t.s.
    long best_committed_at = 0L; // for finding write snapshot isolated t.s.

    for (KeyValue kv : list_committed_at) {
      long cell_timestamp = kv.getTimestamp();
      long cell_committed_at = Bytes.toLong(kv.getValue());

      LOG.trace("ts " + cell_timestamp + ": " + "(committed-at cell = "
          + cell_committed_at + ")");

      // If row is uncommitted, suspect that it should be committed
      if (cell_timestamp != ref_timestamp
          && cell_committed_at == Schema.TIMESTAMP_NULL_LONG) {
        // Get the state of its parent txn from Timestamp-Log
        Result cell_startTS_res = timestamp_log_table.get(new Get(
            timestampToKey(cell_timestamp)));
        byte[] otherTxnState = cell_startTS_res.getColumnLatest(
            Schema.FAMILY_HACID, Schema.QUALIFIER_TXN_STATE).getValue();

        // State of parent txn active (= before commit/abort decision)
        if (Bytes.equals(otherTxnState, Schema.STATE_ACTIVE)) {
          // Scan from cell_timestamp to ref_timestamp in Timestamp Log
          // and find the row that has start-ts = cell_timestamp
          SingleColumnValueFilter filter = new SingleColumnValueFilter(
              Schema.FAMILY_HACID, Schema.QUALIFIER_START_TIMESTAMP,
              CompareFilter.CompareOp.EQUAL, Bytes.toBytes(cell_timestamp));
          filter.setFilterIfMissing(true);
          Scan scan = new Scan(timestampToKey(ref_timestamp - 1L),
              timestampToKey(cell_timestamp));
          int cacheSize = ((ref_timestamp - cell_timestamp) < SCAN_TIMESTAMP_MAX_CACHE) ? ((int) (ref_timestamp - cell_timestamp))
              : SCAN_TIMESTAMP_MAX_CACHE;
          scan.setCaching(cacheSize);
          scan.setFilter(filter);
          ResultScanner scanner = timestamp_log_table.getScanner(scan);
          Result cell_endTS_res = scanner.next();
          scanner.close();

          // If found that end timestamp
          if (cell_endTS_res != null) {
            // do recovery from canCommit() onwards (like decideCommitOrAbort)
            HAcidTxn cellTxn = HAcidTxn.restore(this, cell_endTS_res);
            if (cellTxn.decideCommitOrAbort() == true) {
              cell_committed_at = cellTxn.getEndTimestamp();
            }
          }
          else { // not found that commit timestamp
            // Do nothing because the parent of cell_timestamp will
            // have commit timestamp larger than the ref_timestamp
            continue;
          }
        }
        else { // Decision abort/commit is already taken, so do recovery
          Result cell_endTS_res = timestamp_log_table.get(new Get(
              timestampToKey(cell_startTS_res.getColumnLatest(
                  Schema.FAMILY_HACID, Schema.QUALIFIER_END_TIMESTAMP)
                  .getValue())));
          HAcidTxn cellTxn = HAcidTxn.restore(this, cell_endTS_res);

          // if decision was abort, then rollback
          if (Bytes.equals(otherTxnState, Schema.STATE_ABORTED)) {
            cellTxn.rollbackWrites();
            continue;
          }
          // if decision was commit, then commit the operations
          else if (Bytes.equals(otherTxnState, Schema.STATE_COMMITTED)) {
            cellTxn.rollforwardWrites();
            cell_committed_at = cellTxn.getEndTimestamp();
          }
        }
      }

      if (
      // ( // txn is allowed to read its own modifications
      // cell_timestamp == ref_timestamp
      // )
      // ||
      ( // for Snapshot Isolation
      cell_timestamp < ref_timestamp
          && cell_committed_at != Schema.TIMESTAMP_NULL_LONG && cell_committed_at < ref_timestamp)) {
        if (isolation == IsolationLevel.SNAPSHOT_ISOLATION) {
          LOG.trace(cell_timestamp + " is the Snapshot Isolated Read Timestamp");
          return hacidget.collectReadData(res, cell_timestamp);
        }
        else if (isolation == IsolationLevel.WRITE_SNAPSHOT_ISOLATION) {
          if (cell_committed_at > best_committed_at) {
            best_timestamp = cell_timestamp;
            best_committed_at = cell_committed_at;
          }
        }
      }
    }// end for

    if (isolation == IsolationLevel.WRITE_SNAPSHOT_ISOLATION
        && best_timestamp > 0) {
      LOG.trace(best_timestamp
          + " is the Write Snapshot Isolated Read Timestamp");
      return hacidget.collectReadData(res, best_timestamp);
    }

    // Something unusual happened:
    return (new Result(new LinkedList<KeyValue>()));
  }

  /**
   * Retrieves data of a timestamp from the Timestamp Log.
   * 
   * @param timestamp
   *          the timestamp itself
   * @return
   */
  Result getTimestampData(long timestamp) throws IOException {
    Get g = new Get(timestampToKey(timestamp));
    return timestamp_log_table.get(g);
  }

  /**
   * Minimum time, in milliseconds, to wait when a previous transaction with
   * state 'active' is encountered, while checking if a subsequent transaction
   * can commit.
   */
  public static long MIN_TIMEOUT_WAIT_PREVIOUS_ACTIVE_TXN = 128L;

  /**
   * Maximum time, in milliseconds, to wait when a previous transaction with
   * state 'active' is encountered, while checking if a subsequent transaction
   * can commit.
   */
  public static long MAX_TIMEOUT_WAIT_PREVIOUS_ACTIVE_TXN = 16384L;

  /**
   * Number of attempts to recheck for transaction conflicts when previous
   * active transactions are found.
   */
  public static final int DEFAULT_CONFLICT_TEST_ATTEMPTS = 5;

  boolean canCommit(HAcidTxn txn) throws IOException {
    return canCommit(txn, DEFAULT_CONFLICT_TEST_ATTEMPTS);
  }

  /**
   * Tells whether HAcid can commit or not the given transaction, i.e., whether
   * the transaction <code>txn</code> conflicts with some other registered
   * transaction.
   * 
   * @param txn
   * @param attempts
   *          Number of attempts to recheck for transaction conflicts when
   *          previous active transactions are found.
   * @return
   * @throws IOException
   * @throws InterruptedException
   */
  boolean canCommit(HAcidTxn txn, int attempts) throws IOException {
    LOG.info("Checking for conflicts" + txn.toStringHash());

    // No timestamps between txn's start_ts and end_ts
    if (txn.getStartTimestamp() + 1L == txn.getEndTimestamp()) {
      return true;
    }

    int fixed_attempts = (attempts >= 1) ? attempts : 1;

    // From the Timestamp Log, get the end timestamp row of the current txn
    Get g = new Get(timestampToKey(txn.getEndTimestamp()));
    if (isolation == IsolationLevel.SNAPSHOT_ISOLATION) {
      g.addFamily(Schema.FAMILY_WRITESET);
    }
    else if (isolation == IsolationLevel.WRITE_SNAPSHOT_ISOLATION) {
      g.addFamily(Schema.FAMILY_READSET);
    }
    // this will either hold the writeset or the readset,
    // according to the isolation level:
    Result currentTxnComparisonSet = timestamp_log_table.get(g);

    // Scan previous possibly conflicting transactions before 'txn'
    SingleColumnValueFilter filter = new SingleColumnValueFilter(
        Schema.FAMILY_HACID, Schema.QUALIFIER_TS_TYPE,
        CompareFilter.CompareOp.EQUAL, Schema.TYPE_END);
    filter.setFilterIfMissing(true);
    Scan scan = new Scan(timestampToKey(txn.getEndTimestamp() - 1L),
        timestampToKey(txn.getStartTimestamp()));
    int cacheSize = ((txn.getEndTimestamp() - txn.getStartTimestamp()) < SCAN_TIMESTAMP_MAX_CACHE) ? ((int) (txn
        .getEndTimestamp() - txn.getStartTimestamp()))
        : SCAN_TIMESTAMP_MAX_CACHE;
    scan.setCaching(cacheSize);
    scan.setFilter(filter); // get only timestamp rows that are end timestamps
    Result previousTxn;
    long timeout = MIN_TIMEOUT_WAIT_PREVIOUS_ACTIVE_TXN;

    for (int i = 0; i < fixed_attempts; i++) {
      ResultScanner scanner = timestamp_log_table.getScanner(scan);
      // No other transaction committed in my life span, hence I can commit
      if ((previousTxn = scanner.next()) == null) {
        LOG.debug("No other txn committed during " + txn.toString()
            + ", so it can commit" + txn.toStringHash());
        return true;
      }
      // There might be conflicts:
      else {
        boolean foundActiveConflictingTxn = false;
        do {
          // Fetch state of the previous txn from its start-ts row
          byte[] stateOfPrevious = timestamp_log_table
              .get(
                  new Get(timestampToKey(previousTxn.getColumnLatest(
                      Schema.FAMILY_HACID, Schema.QUALIFIER_START_TIMESTAMP)
                      .getValue())).addColumn(Schema.FAMILY_HACID,
                      Schema.QUALIFIER_TXN_STATE))
              .getColumnLatest(Schema.FAMILY_HACID, Schema.QUALIFIER_TXN_STATE)
              .getValue();

          // Search for conflicts only with 'committed' and 'active'
          // transactions
          if (!Bytes.equals(stateOfPrevious, Schema.STATE_ABORTED)) {
            intersectionCheck: // Compare the previous and the current txns
            for (KeyValue kv : currentTxnComparisonSet.raw()) {
              LOG.trace("current txn has "
                  + Bytes.toString(Schema.FAMILY_WRITESET) + ":"
                  + Bytes.toString(kv.getQualifier()) + txn.toStringHash());
              if (previousTxn.containsColumn(Schema.FAMILY_WRITESET,
                  kv.getQualifier())) {
                if (Bytes.equals(stateOfPrevious, Schema.STATE_COMMITTED)) {
                  LOG.warn("TRANSACTION CONFLICT: Both transactions "
                      + HAcidTxn.lightRestore(this, previousTxn).toString()
                      + " and " + txn.toString() + " modify the common cell "
                      + Bytes.toString(kv.getQualifier())
                      + ", so the second txn must abort." + txn.toStringHash());
                  scanner.close();
                  return false; // Found a conflict so cannot commit
                }
                else if (Bytes.equals(stateOfPrevious, Schema.STATE_ACTIVE)) {
                  LOG.warn("Found a previous active conflicting transaction "
                      + HAcidTxn.lightRestore(this, previousTxn).toString()
                      + txn.toStringHash());
                  foundActiveConflictingTxn = true; // Found an active
                                                    // conflicting txn
                  break intersectionCheck;
                }
              }
            }
          }
        }
        while ((previousTxn = scanner.next()) != null);

        // No conflicts found and no previous active conflicting txn, so can
        // commit
        if (!foundActiveConflictingTxn) {
          LOG.debug("No conflicting transactions found, so it can commit"
              + txn.toStringHash());
          scanner.close();
          return true;
        }
      }
      scanner.close();

      // Wait until some previous 'active' transaction gets decided:
      LOG.debug("Waiting " + timeout + " ms until previous active "
          + "conflicting txns get decided" + txn.toStringHash());
      try {
        Thread.sleep(timeout);
      }
      catch (InterruptedException ex) {
        LOG.warn(ex.toString());
      }
      timeout *= 2;
      if (timeout > MAX_TIMEOUT_WAIT_PREVIOUS_ACTIVE_TXN) {
        timeout = MAX_TIMEOUT_WAIT_PREVIOUS_ACTIVE_TXN;
      }
    }

    LOG.debug("Last check for conflicts, after the timeout"
        + txn.toStringHash());
    // Try to check again
    ResultScanner scannerAgain = timestamp_log_table.getScanner(scan);
    while ((previousTxn = scannerAgain.next()) != null) {
      byte[] stateOfPrevious = timestamp_log_table
          .get(
              new Get(timestampToKey(previousTxn.getColumnLatest(
                  Schema.FAMILY_HACID, Schema.QUALIFIER_START_TIMESTAMP)
                  .getValue())).addColumn(Schema.FAMILY_HACID,
                  Schema.QUALIFIER_TXN_STATE))
          .getColumnLatest(Schema.FAMILY_HACID, Schema.QUALIFIER_TXN_STATE)
          .getValue();

      // Conflicts with 'active' or 'committed' transactions are enough to abort
      if (!Bytes.equals(stateOfPrevious, Schema.STATE_ABORTED)) {
        for (KeyValue kv : currentTxnComparisonSet.raw()) {
          if (previousTxn.containsColumn(Schema.FAMILY_WRITESET,
              kv.getQualifier())) {
            LOG.warn("TRANSACTION CONFLICT: Both transactions "
                + HAcidTxn.lightRestore(this, previousTxn).toString() + " and "
                + txn.toString() + " modify the common cell "
                + Bytes.toString(kv.getQualifier())
                + ", so the second txn must abort." + txn.toStringHash());
            scannerAgain.close();
            return false; // Found a conflict so cannot commit
          }
        }
      }
    }

    // No conflicts found, so can commit
    LOG.debug("No conflicting transactions found, so it can commit");
    scannerAgain.close();
    return true;
  }

  /**
   * Sets the state of the specified transaction in the Timestamp Log.
   * 
   * @param txn
   * @param newState
   * @throws IOException
   */
  private void setDecisionState(HAcidTxn txn, byte[] newState)
      throws IOException, StateDisagreementException {
    // Change the state in the Timestamp Log
    Put tslog_put = new Put(timestampToKey(txn.getStartTimestamp()));
    tslog_put.add(Schema.FAMILY_HACID, Schema.QUALIFIER_TXN_STATE, newState);
    tslog_put.add(Schema.FAMILY_HACID, Schema.QUALIFIER_END_TIMESTAMP,
        Bytes.toBytes(txn.getEndTimestamp()));
    if (timestamp_log_table.checkAndPut(
        timestampToKey(txn.getStartTimestamp()), Schema.FAMILY_HACID,
        Schema.QUALIFIER_TXN_STATE, Schema.STATE_ACTIVE, tslog_put) == false) {
      throw new StateDisagreementException();
    }
  }

  /**
   * Marks the specified transaction as "committed" in the Timestamp Log.
   * 
   * @param txn
   * @throws IOException
   */
  void setStateCommitted(HAcidTxn txn) throws IOException,
      StateDisagreementException {
    LOG.info("Txn " + txn.toString() + " is COMMITTED");
    setDecisionState(txn, Schema.STATE_COMMITTED);
  }

  /**
   * Marks the specified transaction as "aborted" in the Timestamp Log.
   * 
   * @param txn
   * @throws IOException
   */
  void setStateAborted(HAcidTxn txn) throws IOException,
      StateDisagreementException {
    LOG.info("Txn " + txn.toString() + " is ABORTED");
    setDecisionState(txn, Schema.STATE_ABORTED);
  }

  /**
   * Tells whether or not HAcid is installed at the HBase site referred by
   * config.
   * 
   * @param config
   * @return Whether or not HAcid is installed at the HBase site
   * @throws Exception
   */
  public static boolean isInstalled(Configuration config) throws Exception {
    if (config == null) {
      LOG.error("No connection to HBase. Configuration is null");
    }

    HBaseAdmin admin = new HBaseAdmin(config);

    LOG.trace("Checking whether HAcid is installed in " + config.toString());

    return admin.tableExists(Schema.TABLE_TIMESTAMP_LOG);
  }

  /**
   * Sets up a HBase data store to use HAcid. Creates the HAcid metadata tables,
   * and prepares each user table to be used by HAcid.
   * 
   * @param config
   *          The configuration data of the HBase site where HAcid metadata
   *          tables will stay.
   * @param userTables
   *          A Collection of user tables to be prepared for use in HAcid.
   * @throws Exception
   */
  private static void install(Configuration config,
      Collection<HTable> allUserTables) throws Exception {
    if (config == null) {
      LOG.error("No connection to HBase. Configuration is null");
    }

    HBaseAdmin admin = new HBaseAdmin(config);

    LOG.info("Installing HAcid in " + config.toString() + " ...");

    // Opens the Timestamp Log table, and first initializes it if necessary
    if (!admin.tableExists(Schema.TABLE_TIMESTAMP_LOG)) {
      LOG.debug("Creating HAcid Timestamp Log table");
      HTableDescriptor logDescriptor = new HTableDescriptor(
          Schema.TABLE_TIMESTAMP_LOG);
      HColumnDescriptor hacidFamily = new HColumnDescriptor(Schema.FAMILY_HACID);
      HColumnDescriptor writesetFamily = new HColumnDescriptor(
          Schema.FAMILY_WRITESET);
      HColumnDescriptor readsetFamily = new HColumnDescriptor(
          Schema.FAMILY_READSET);
      hacidFamily.setMaxVersions(1);
      writesetFamily.setMaxVersions(1);
      readsetFamily.setMaxVersions(1);
      logDescriptor.addFamily(hacidFamily);
      logDescriptor.addFamily(writesetFamily);
      logDescriptor.addFamily(readsetFamily);
      admin.createTable(logDescriptor);
      HTable logTable = new HTable(config, Schema.TABLE_TIMESTAMP_LOG);

      // Makes the first timestamp as used (because user tables might be
      // initialized with this)
      Put firstPut = new Put(timestampToKey(Schema.TIMESTAMP_INITIAL_LONG));
      firstPut.add(Schema.FAMILY_HACID, Schema.QUALIFIER_TS_TYPE,
          Schema.TYPE_END);
      firstPut.add(Schema.FAMILY_HACID, Schema.QUALIFIER_START_TIMESTAMP,
          Schema.TIMESTAMP_INITIAL);
      firstPut.add(Schema.FAMILY_HACID, Schema.QUALIFIER_END_TIMESTAMP,
          Schema.TIMESTAMP_INITIAL);
      firstPut.add(Schema.FAMILY_HACID, Schema.QUALIFIER_TXN_STATE,
          Schema.STATE_COMMITTED);
      logTable.put(firstPut);
    }

    // Prepares all user tables
    if (allUserTables != null) {
      LOG.debug("Preparing a collection of user tables for use in HAcid");
      for (HTable usertable : allUserTables) {
        prepareUserTable(usertable);
      }
    }
  }

  /**
   * Removes the HAcid metadata tables from HBase.
   * 
   * @param config
   *          The configuration data of the HBase site where HAcid metadata
   *          tables are.
   * @throws Exception
   */
  public static void uninstall(Configuration config) throws Exception {
    if (config == null) {
      LOG.error("No connection to HBase. Configuration is null");
    }

    LOG.debug("Uninstalling HAcid from " + config.toString() + " ...");

    HBaseAdmin admin = new HBaseAdmin(config);
    admin.disableTable(Schema.TABLE_TIMESTAMP_LOG);
    admin.deleteTable(Schema.TABLE_TIMESTAMP_LOG);
  }

  /**
   * Tells whether or not the user table given is prepared for HAcid use.
   * 
   * @param usertable
   * @return
   * @throws Exception
   */
  static boolean isUserTablePrepared(HTable usertable) throws IOException {
    boolean result = (usertable.getTableDescriptor()
        .hasFamily(Schema.FAMILY_HACID));
    LOG.debug("Is user table \'" + Bytes.toString(usertable.getTableName())
        + "\' prepared? " + result);
    return result;
  }

  /**
   * Prepares the user table for HAcid transactions. The user table should not
   * use a column family named 'HAcid'.
   * 
   * ATTENTION: this deletes all previous timestamps of all cells, and keeps the
   * latest timestamp.
   * 
   * @param usertable
   * @throws IOException
   * @throws Exception
   */
  static void prepareUserTable(HTable usertable) throws IOException {
    LOG.info("Preparing user table \'"
        + Bytes.toString(usertable.getTableName()) + "\' for use with HAcid.");

    // Make sure this user table is not already prepared
    if (usertable.getTableDescriptor().getFamily(Schema.FAMILY_HACID) != null) {
      LOG.warn("User table \'" + Bytes.toString(usertable.getTableName())
          + "\' is already prepared for HAcid or uses a column family "
          + "with the special HAcid keyword.");
      return;
    }

    // Prepares admin settings
    if (usertable.getConfiguration() == null) {
      LOG.error("No connection to the HBase site that contains the user "
          + "table. Configuration is null");
    }
    HBaseAdmin admin = new HBaseAdmin(usertable.getConfiguration());
    byte[] usertableName = usertable.getTableName();

    // Set timestamp of all cells to TIMESTAMP_INITIAL
    {
      // Remove all previous data, and remember that data
      Scan scan = new Scan();
      scan.setTimeRange(0L, HConstants.LATEST_TIMESTAMP);
      scan.setMaxVersions();
      ResultScanner scanner = usertable.getScanner(scan);
      Result result;
      while ((result = scanner.next()) != null) {
        LinkedList<Put> reinsertSet = new LinkedList<Put>();
        for (KeyValue kv : result.raw()) {
          if (kv.getTimestamp() != Schema.TIMESTAMP_INITIAL_LONG) {
            Put put = new Put(result.getRow(), Schema.TIMESTAMP_INITIAL_LONG);
            put.add(kv.getFamily(), kv.getQualifier(), kv.getValue());
            reinsertSet.add(put);
            Delete deleteThisVersion = new Delete(result.getRow());
            deleteThisVersion.deleteColumn(kv.getFamily(), kv.getQualifier(),
                kv.getTimestamp());
            usertable.delete(deleteThisVersion);
          }
        }
        // Reinsert all previous data, but with new timestamp
        for (Put reinsert : reinsertSet) {
          usertable.put(reinsert);
        }
        reinsertSet.clear();
        reinsertSet = null;
      }
      scanner.close();
    }

    // Disable the table (necessary before a modify())
    admin.disableTable(usertableName);

    // Set max versions (of each col.family)
    HTableDescriptor tableDescriptor = new HTableDescriptor(usertableName);
    for (HColumnDescriptor previousFamily : admin.getTableDescriptor(
        usertableName).getColumnFamilies()) {
      HColumnDescriptor newFamily = new HColumnDescriptor(previousFamily);
      newFamily.setMaxVersions(Schema.MAX_VERSIONS_USERTABLE);
      tableDescriptor.addFamily(newFamily);
    }

    // Insert new column family 'HAcid'
    HColumnDescriptor hacidFamily = new HColumnDescriptor(Schema.FAMILY_HACID);
    hacidFamily.setMaxVersions(Schema.MAX_VERSIONS_USERTABLE);
    tableDescriptor.addFamily(hacidFamily);

    // Apply the changes to the schema
    admin.modifyTable(usertableName, tableDescriptor);

    // Enable the table
    admin.enableTable(usertableName);

    // Insert the committed-at column for each row
    {
      Scan scan = new Scan();
      ResultScanner scanner = usertable.getScanner(scan);
      Result result;
      while ((result = scanner.next()) != null) {
        Put committed_at_put = new Put(result.getRow(),
            Schema.TIMESTAMP_INITIAL_LONG);
        committed_at_put.add(Schema.FAMILY_HACID,
            Schema.QUALIFIER_USERTABLE_COMMITTED_AT, Schema.TIMESTAMP_INITIAL);
        usertable.put(committed_at_put);
      }
      scanner.close();
    }
    usertable.flushCommits();
  }

}
